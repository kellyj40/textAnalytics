{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Question 1 - Use NLTK package, normalise text file with tokenizer method,\n",
    "    Use the postagger and look for inaccuracies'''\n",
    "\n",
    "# Imports\n",
    "import nltk\n",
    "\n",
    "'''\n",
    "Another method to read in from file\n",
    "file = open(\"directory/to/file\")\n",
    "raw_text = file.read()\n",
    "raw_text\n",
    "'''\n",
    "\n",
    "# Part a\n",
    "\n",
    "# Function to read from text file and return string\n",
    "def read_text_from_file(file_path_name):\n",
    "    # Open the file and read in the text\n",
    "    with open(file_path_name) as file:\n",
    "        text = file.read()\n",
    "        file.close()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paragraph = read_text_from_file(\"nltk_file.txt\")\n",
    "\n",
    "# normalise the data by making lower case\n",
    "paragraph = paragraph.lower()\n",
    "\n",
    "token_words = nltk.word_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'was', 'here', 'to', 'test', 'the', 'irregular', 'stemming', 'words', '.', 'there', 'were', 'four', 'people', 'and', 'that', 'is', 'half', 'the', 'amount', 'of', 'last', 'week', ',', 'there', 'was', 'also', 'a', 'dozen', 'people', 'there', '.', 'he', 'said', '``', 'after', 'he', 'would', 'train', 'and', 'get', 'the', 'train', 'home', \"''\", '.', 'no', 'previous', 'convictions', ',', 'will', 'the', 'no', 'be', 'lost', 'and', 'change', 'context', '?', 'he', 'said', 'that', 'miss', \"o'neill\", 'might', 'miss', 'the', 'train', ',', 'but', 'she', 'did', \"n't\", '.', 'he', 'was', '12', 'years-old', ',', 'born', 'at', '12:53', 'on', 'the', '12/5/2017', '12-5-2017', 'and', '12th', 'december', '2017', 'this', 'well-known', 'chapter', 'in', 'the', 'u.k', 'is', 'divided', 'into', 'up-to-date', 'sections', 'that', 'skip', 'between', 'two', 'quite', 'different', 'styles', 'for', 'u.c.c', 'students', 'and', 'john', \"o'kelly\", '.', 'it', \"'s\", 'in', 'the', '``', 'computing', 'with', 'language', \"''\", 'sections', 'we', 'will', 'take', 'on', 'some', 'linguistically', 'motivated', 'programming', 'tasks', 'without', 'necessarily', 'explaining', 'how', 'they', 'work', '.', 'in', 'the', '``', 'closer', 'look', 'at', 'python', \"''\", 'sections', 'we', 'will', 'systematically', 'review', 'key', 'programming', 'concepts', '.', 'we', \"'ll\", 'flag', 'the', 'two', 'styles', 'in', 'the', 'section', 'titles', ',', 'but', 'later', 'chapters', 'will', 'mix', 'both', 'styles', 'without', 'being', 'so', 'up-front', 'about', 'it', '.', 'we', 'hope', 'this', 'style', 'of', 'introduction', 'gives', 'you', 'an', 'authentic', 'taste', 'of', 'what', 'will', 'come', 'later', ',', 'while', 'covering', 'a', 'range', 'of', 'elementary', 'concepts', 'in', 'linguistics', 'and', 'computer', 'science', '.', 'if', 'you', 'have', 'basic', 'familiarity', 'with', 'both', 'areas', ',', 'you', 'can', 'skip', 'to', '5', ';', 'we', 'will', 'repeat', 'any', 'important', 'points', 'in', 'later', 'chapters', ',', 'and', 'if', 'you', 'miss', 'anything', 'you', 'can', 'easily', 'consult', 'the', 'online', 'reference', 'material', 'at', 'http', ':', '//nltk.org/genindex.html', 'and', 'nltk.org/genindex.html', 'and', 'http', ':', '//www.nltk.org/genindex.html', '.', 'if', 'the', 'material', 'is', 'completely', 'new', 'to', 'you', ',', 'this', 'chapter', 'will', 'raise', 'more', 'questions', 'than', 'it', 'answers', ',', 'questions', 'that', 'are', 'addressed', 'in', 'the', 'rest', 'of', 'this', 'book', '.']\n"
     ]
    }
   ],
   "source": [
    "# Oddities\n",
    "# Speech marks are seperated from the words they follow into there own\n",
    "# We'll is split into we and 'll and It's is split into it and 's\n",
    "# Yet 'O'Kelly' is kept as one word\n",
    "# hyphan words are kept together eg. ice-skate\n",
    "# U.C.C was also kept as one word, yet when\n",
    "# The link http://www.nltk.org/genindex.html was split into 3 'words'\n",
    "print(token_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 'NN'), ('was', 'VBD'), ('here', 'RB'), ('to', 'TO'), ('test', 'VB'), ('the', 'DT'), ('irregular', 'JJ'), ('stemming', 'NN'), ('words', 'NNS'), ('.', '.'), ('there', 'EX'), ('were', 'VBD'), ('four', 'CD'), ('people', 'NNS'), ('and', 'CC'), ('that', 'DT'), ('is', 'VBZ'), ('half', 'PDT'), ('the', 'DT'), ('amount', 'NN'), ('of', 'IN'), ('last', 'JJ'), ('week', 'NN'), (',', ','), ('there', 'EX'), ('was', 'VBD'), ('also', 'RB'), ('a', 'DT'), ('dozen', 'NN'), ('people', 'NNS'), ('there', 'RB'), ('.', '.'), ('he', 'PRP'), ('said', 'VBD'), ('``', '``'), ('after', 'IN'), ('he', 'PRP'), ('would', 'MD'), ('train', 'VB'), ('and', 'CC'), ('get', 'VB'), ('the', 'DT'), ('train', 'NN'), ('home', 'NN'), (\"''\", \"''\"), ('.', '.'), ('no', 'DT'), ('previous', 'JJ'), ('convictions', 'NNS'), (',', ','), ('will', 'MD'), ('the', 'DT'), ('no', 'DT'), ('be', 'VB'), ('lost', 'VBN'), ('and', 'CC'), ('change', 'NN'), ('context', 'NN'), ('?', '.'), ('he', 'PRP'), ('said', 'VBD'), ('that', 'IN'), ('miss', 'NN'), (\"o'neill\", 'NN'), ('might', 'MD'), ('miss', 'VB'), ('the', 'DT'), ('train', 'NN'), (',', ','), ('but', 'CC'), ('she', 'PRP'), ('did', 'VBD'), (\"n't\", 'RB'), ('.', '.'), ('he', 'PRP'), ('was', 'VBD'), ('12', 'CD'), ('years-old', 'JJ'), (',', ','), ('born', 'VBN'), ('at', 'IN'), ('12:53', 'CD'), ('on', 'IN'), ('the', 'DT'), ('12/5/2017', 'CD'), ('12-5-2017', 'JJ'), ('and', 'CC'), ('12th', 'CD'), ('december', 'NN'), ('2017', 'CD'), ('this', 'DT'), ('well-known', 'JJ'), ('chapter', 'NN'), ('in', 'IN'), ('the', 'DT'), ('u.k', 'NN'), ('is', 'VBZ'), ('divided', 'VBN'), ('into', 'IN'), ('up-to-date', 'JJ'), ('sections', 'NNS'), ('that', 'WDT'), ('skip', 'VBP'), ('between', 'IN'), ('two', 'CD'), ('quite', 'RB'), ('different', 'JJ'), ('styles', 'NNS'), ('for', 'IN'), ('u.c.c', 'JJ'), ('students', 'NNS'), ('and', 'CC'), ('john', 'NN'), (\"o'kelly\", 'RB'), ('.', '.'), ('it', 'PRP'), (\"'s\", 'VBZ'), ('in', 'IN'), ('the', 'DT'), ('``', '``'), ('computing', 'NN'), ('with', 'IN'), ('language', 'NN'), (\"''\", \"''\"), ('sections', 'NNS'), ('we', 'PRP'), ('will', 'MD'), ('take', 'VB'), ('on', 'IN'), ('some', 'DT'), ('linguistically', 'RB'), ('motivated', 'VBN'), ('programming', 'NN'), ('tasks', 'NNS'), ('without', 'IN'), ('necessarily', 'RB'), ('explaining', 'VBG'), ('how', 'WRB'), ('they', 'PRP'), ('work', 'VBP'), ('.', '.'), ('in', 'IN'), ('the', 'DT'), ('``', '``'), ('closer', 'JJ'), ('look', 'NN'), ('at', 'IN'), ('python', 'NN'), (\"''\", \"''\"), ('sections', 'NNS'), ('we', 'PRP'), ('will', 'MD'), ('systematically', 'RB'), ('review', 'VB'), ('key', 'JJ'), ('programming', 'VBG'), ('concepts', 'NNS'), ('.', '.'), ('we', 'PRP'), (\"'ll\", 'MD'), ('flag', 'VB'), ('the', 'DT'), ('two', 'CD'), ('styles', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('section', 'NN'), ('titles', 'NNS'), (',', ','), ('but', 'CC'), ('later', 'JJR'), ('chapters', 'NNS'), ('will', 'MD'), ('mix', 'VB'), ('both', 'DT'), ('styles', 'NNS'), ('without', 'IN'), ('being', 'VBG'), ('so', 'RB'), ('up-front', 'JJ'), ('about', 'IN'), ('it', 'PRP'), ('.', '.'), ('we', 'PRP'), ('hope', 'VBP'), ('this', 'DT'), ('style', 'NN'), ('of', 'IN'), ('introduction', 'NN'), ('gives', 'VBZ'), ('you', 'PRP'), ('an', 'DT'), ('authentic', 'JJ'), ('taste', 'NN'), ('of', 'IN'), ('what', 'WP'), ('will', 'MD'), ('come', 'VB'), ('later', 'RB'), (',', ','), ('while', 'IN'), ('covering', 'VBG'), ('a', 'DT'), ('range', 'NN'), ('of', 'IN'), ('elementary', 'JJ'), ('concepts', 'NNS'), ('in', 'IN'), ('linguistics', 'NNS'), ('and', 'CC'), ('computer', 'NN'), ('science', 'NN'), ('.', '.'), ('if', 'IN'), ('you', 'PRP'), ('have', 'VBP'), ('basic', 'JJ'), ('familiarity', 'NN'), ('with', 'IN'), ('both', 'DT'), ('areas', 'NNS'), (',', ','), ('you', 'PRP'), ('can', 'MD'), ('skip', 'VB'), ('to', 'TO'), ('5', 'CD'), (';', ':'), ('we', 'PRP'), ('will', 'MD'), ('repeat', 'VB'), ('any', 'DT'), ('important', 'JJ'), ('points', 'NNS'), ('in', 'IN'), ('later', 'JJ'), ('chapters', 'NNS'), (',', ','), ('and', 'CC'), ('if', 'IN'), ('you', 'PRP'), ('miss', 'VBP'), ('anything', 'NN'), ('you', 'PRP'), ('can', 'MD'), ('easily', 'RB'), ('consult', 'VB'), ('the', 'DT'), ('online', 'JJ'), ('reference', 'NN'), ('material', 'NN'), ('at', 'IN'), ('http', 'NN'), (':', ':'), ('//nltk.org/genindex.html', 'NN'), ('and', 'CC'), ('nltk.org/genindex.html', 'NN'), ('and', 'CC'), ('http', 'NN'), (':', ':'), ('//www.nltk.org/genindex.html', 'NN'), ('.', '.'), ('if', 'IN'), ('the', 'DT'), ('material', 'NN'), ('is', 'VBZ'), ('completely', 'RB'), ('new', 'JJ'), ('to', 'TO'), ('you', 'PRP'), (',', ','), ('this', 'DT'), ('chapter', 'NN'), ('will', 'MD'), ('raise', 'VB'), ('more', 'JJR'), ('questions', 'NNS'), ('than', 'IN'), ('it', 'PRP'), ('answers', 'VBZ'), (',', ','), ('questions', 'NNS'), ('that', 'WDT'), ('are', 'VBP'), ('addressed', 'VBN'), ('in', 'IN'), ('the', 'DT'), ('rest', 'NN'), ('of', 'IN'), ('this', 'DT'), ('book', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "pos_tagger_words = nltk.pos_tag(token_words)\n",
    "print(pos_tagger_words )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 'NN'), ('was', 'VBD'), ('here', 'RB'), ('to', 'TO'), ('test', 'VB'), ('the', 'DT'), ('irregular', 'JJ'), ('stemming', 'NN'), ('words', 'NNS'), ('.', '.'), ('there', 'EX'), ('were', 'VBD'), ('four', 'CD'), ('people', 'NNS'), ('and', 'CC'), ('that', 'DT'), ('is', 'VBZ'), ('half', 'PDT'), ('the', 'DT'), ('amount', 'NN'), ('of', 'IN'), ('last', 'JJ'), ('week', 'NN'), (',', ','), ('there', 'EX'), ('was', 'VBD'), ('also', 'RB'), ('a', 'DT'), ('dozen', 'NN'), ('people', 'NNS'), ('there', 'RB'), ('.', '.'), ('he', 'PRP'), ('said', 'VBD'), ('``', '``'), ('after', 'IN'), ('he', 'PRP'), ('would', 'MD'), ('train', 'VB'), ('and', 'CC'), ('get', 'VB'), ('the', 'DT'), ('train', 'NN'), ('home', 'NN'), (\"''\", \"''\"), ('.', '.'), ('no', 'DT'), ('previous', 'JJ'), ('convictions', 'NNS'), (',', ','), ('will', 'MD'), ('the', 'DT'), ('no', 'DT'), ('be', 'VB'), ('lost', 'VBN'), ('and', 'CC'), ('change', 'NN'), ('context', 'NN'), ('?', '.'), ('he', 'PRP'), ('said', 'VBD'), ('that', 'IN'), ('miss', 'NN'), (\"o'neill\", 'NN'), ('might', 'MD'), ('miss', 'VB'), ('the', 'DT'), ('train', 'NN'), (',', ','), ('but', 'CC'), ('she', 'PRP'), ('did', 'VBD'), (\"n't\", 'RB'), ('.', '.'), ('he', 'PRP'), ('was', 'VBD'), ('12', 'CD'), ('years-old', 'JJ'), (',', ','), ('born', 'VBN'), ('at', 'IN'), ('12:53', 'CD'), ('on', 'IN'), ('the', 'DT'), ('12/5/2017', 'CD'), ('12-5-2017', 'JJ'), ('and', 'CC'), ('12th', 'CD'), ('december', 'NN'), ('2017', 'CD'), ('this', 'DT'), ('well-known', 'JJ'), ('chapter', 'NN'), ('in', 'IN'), ('the', 'DT'), ('u.k', 'NN'), ('is', 'VBZ'), ('divided', 'VBN'), ('into', 'IN'), ('up-to-date', 'JJ'), ('sections', 'NNS'), ('that', 'WDT'), ('skip', 'VBP'), ('between', 'IN'), ('two', 'CD'), ('quite', 'RB'), ('different', 'JJ'), ('styles', 'NNS'), ('for', 'IN'), ('u.c.c', 'JJ'), ('students', 'NNS'), ('and', 'CC'), ('john', 'NN'), (\"o'kelly\", 'RB'), ('.', '.'), ('it', 'PRP'), (\"'s\", 'VBZ'), ('in', 'IN'), ('the', 'DT'), ('``', '``'), ('computing', 'NN'), ('with', 'IN'), ('language', 'NN'), (\"''\", \"''\"), ('sections', 'NNS'), ('we', 'PRP'), ('will', 'MD'), ('take', 'VB'), ('on', 'IN'), ('some', 'DT'), ('linguistically', 'RB'), ('motivated', 'VBN'), ('programming', 'NN'), ('tasks', 'NNS'), ('without', 'IN'), ('necessarily', 'RB'), ('explaining', 'VBG'), ('how', 'WRB'), ('they', 'PRP'), ('work', 'VBP'), ('.', '.'), ('in', 'IN'), ('the', 'DT'), ('``', '``'), ('closer', 'JJ'), ('look', 'NN'), ('at', 'IN'), ('python', 'NN'), (\"''\", \"''\"), ('sections', 'NNS'), ('we', 'PRP'), ('will', 'MD'), ('systematically', 'RB'), ('review', 'VB'), ('key', 'JJ'), ('programming', 'VBG'), ('concepts', 'NNS'), ('.', '.'), ('we', 'PRP'), (\"'ll\", 'MD'), ('flag', 'VB'), ('the', 'DT'), ('two', 'CD'), ('styles', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('section', 'NN'), ('titles', 'NNS'), (',', ','), ('but', 'CC'), ('later', 'JJR'), ('chapters', 'NNS'), ('will', 'MD'), ('mix', 'VB'), ('both', 'DT'), ('styles', 'NNS'), ('without', 'IN'), ('being', 'VBG'), ('so', 'RB'), ('up-front', 'JJ'), ('about', 'IN'), ('it', 'PRP'), ('.', '.'), ('we', 'PRP'), ('hope', 'VBP'), ('this', 'DT'), ('style', 'NN'), ('of', 'IN'), ('introduction', 'NN'), ('gives', 'VBZ'), ('you', 'PRP'), ('an', 'DT'), ('authentic', 'JJ'), ('taste', 'NN'), ('of', 'IN'), ('what', 'WP'), ('will', 'MD'), ('come', 'VB'), ('later', 'RB'), (',', ','), ('while', 'IN'), ('covering', 'VBG'), ('a', 'DT'), ('range', 'NN'), ('of', 'IN'), ('elementary', 'JJ'), ('concepts', 'NNS'), ('in', 'IN'), ('linguistics', 'NNS'), ('and', 'CC'), ('computer', 'NN'), ('science', 'NN'), ('.', '.'), ('if', 'IN'), ('you', 'PRP'), ('have', 'VBP'), ('basic', 'JJ'), ('familiarity', 'NN'), ('with', 'IN'), ('both', 'DT'), ('areas', 'NNS'), (',', ','), ('you', 'PRP'), ('can', 'MD'), ('skip', 'VB'), ('to', 'TO'), ('5', 'CD'), (';', ':'), ('we', 'PRP'), ('will', 'MD'), ('repeat', 'VB'), ('any', 'DT'), ('important', 'JJ'), ('points', 'NNS'), ('in', 'IN'), ('later', 'JJ'), ('chapters', 'NNS'), (',', ','), ('and', 'CC'), ('if', 'IN'), ('you', 'PRP'), ('miss', 'VBP'), ('anything', 'NN'), ('you', 'PRP'), ('can', 'MD'), ('easily', 'RB'), ('consult', 'VB'), ('the', 'DT'), ('online', 'JJ'), ('reference', 'NN'), ('material', 'NN'), ('at', 'IN'), ('http', 'NN'), (':', ':'), ('//nltk.org/genindex.html', 'NN'), ('and', 'CC'), ('nltk.org/genindex.html', 'NN'), ('and', 'CC'), ('http', 'NN'), (':', ':'), ('//www.nltk.org/genindex.html', 'NN'), ('.', '.'), ('if', 'IN'), ('the', 'DT'), ('material', 'NN'), ('is', 'VBZ'), ('completely', 'RB'), ('new', 'JJ'), ('to', 'TO'), ('you', 'PRP'), (',', ','), ('this', 'DT'), ('chapter', 'NN'), ('will', 'MD'), ('raise', 'VB'), ('more', 'JJR'), ('questions', 'NNS'), ('than', 'IN'), ('it', 'PRP'), ('answers', 'VBZ'), (',', ','), ('questions', 'NNS'), ('that', 'WDT'), ('are', 'VBP'), ('addressed', 'VBN'), ('in', 'IN'), ('the', 'DT'), ('rest', 'NN'), ('of', 'IN'), ('this', 'DT'), ('book', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "pos_tagger_words = nltk.pos_tag(token_words)\n",
    "print(pos_tagger_words )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file used was a simple 200 word story, however to get a better understanding, the file was edited to compare different functionalities and how they worked. Dates and special characters were added to see how they would behave. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Question 1 - A \n",
    "\n",
    "When the word_tokernizer() method was used there were some interesting oddities that occured. At a glance, I thought the words were being split by spaces and special characters, such as appostraphies. However not all appostrfies acted the same way. For example we'll was split as \"we\" and \"'ll\" which makes sense as they are two seperate words of we and will, yet the name \"O'Kelly\" was kept as one word. However the tokenizer doesn't just split on the appostraphy to make two words, it was able to knew to split the word \"didn't\" into \"did\" and \"n't\" instead of \"didn\" \"'t\". \n",
    "\n",
    "Special character words such as time \"12:53\" and dates 12/3/2017 were also kept together, yet a web site page such as http://www.nltk.org/ was split into three seperate parts on the same special character ':' into 'http', ':', '//www.nltk.org/'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Question 1 - B\n",
    "\n",
    "The text was normalised by converting all the words to lowercases. Obviously there is alot more to normalising text such as the removal of some special characters. However, when I attempted to do this, some of the context was lost during and would change its meaning, such as the removal of ':' would convert the time 12:53 to 1253.\n",
    "\n",
    "The part-of-speech tagger methed was experimented after normalisation. The pos-tagger was to try and classify different words into their respective lexical categories. It was obvious that this function from the nltk package not only looked at the word itself but also the context in which surrounded it too. \n",
    "\n",
    "Sentences were added into the text to visualise how it would cope with words of the same spelling but with different meaning. \"He said that miss O'Neill might miss the train\" was a good example, here the nltk.pos_tag correctly indicated that the first \"miss\" was a noun(nn) describing a person(O'Neill in this case), and the second was a verb(vb).\n",
    "\n",
    "The POS tagger was also able to convert the spelling of numbers, such as \"four\", to be categorised with CN (Cardinal Number) groups and classify words in which didn't end in s to still be plural (NNS), such as \"people\". \n",
    "\n",
    "In the text, two types of dates were put in, one using \"-\" like \"12-3-2017\" and one using \"/\" like \"12/3/2017\". I originally thought that these would have been classified as the same, yet the date \"12-3-2017\" was classified as a \"JJ\"(Adjective) and the other was a CN(Cardinal number). This was difficult to try and resolve as to why the two were producing different classifications. On further research, it was discovered that JJ was also used for \"numeral\" descriptions as indicated https://stackoverflow.com/questions/15388831/what-are-all-possible-pos-tags-of-nltk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleepi\n",
      "['I', 'was', 'here', 'to', 'test', 'the', 'irregular', 'stemming', 'words', '.', 'There', 'were', 'four', 'people', 'and', 'that', 'is', 'half', 'the', 'amount', 'of', 'last', 'week', ',', 'there', 'was', 'also', 'a', 'dozen', 'people', 'there', '.', 'He', 'said', '``', 'after', 'he', 'would', 'train', 'and', 'get', 'the', 'train', 'home', \"''\", '.', 'No', 'previous', 'convictions', ',', 'will', 'the', 'no', 'be', 'lost', 'and', 'change', 'context', '?', 'He', 'said', 'that', 'miss', \"O'Neill\", 'might', 'miss', 'the', 'train', ',', 'but', 'she', 'did', \"n't\", '.', 'He', 'was', '12', 'years-old', ',', 'born', 'at', '12:53', 'on', 'the', '12/5/2017', '12-5-2017', 'and', '12th', 'December', '2017', 'This', 'well-known', 'chapter', 'in', 'the', 'U.K', 'is', 'divided', 'into', 'up-to-date', 'sections', 'that', 'skip', 'between', 'two', 'quite', 'different', 'styles', 'for', 'U.C.C', 'students', 'and', 'John', \"O'Kelly\", '.', 'It', \"'s\", 'in', 'the', '``', 'computing', 'with', 'language', \"''\", 'sections', 'we', 'will', 'take', 'on', 'some', 'linguistically', 'motivated', 'programming', 'tasks', 'without', 'necessarily', 'explaining', 'how', 'they', 'work', '.', 'In', 'the', '``', 'closer', 'look', 'at', 'Python', \"''\", 'sections', 'we', 'will', 'systematically', 'review', 'key', 'programming', 'concepts', '.', 'We', \"'ll\", 'flag', 'the', 'two', 'styles', 'in', 'the', 'section', 'titles', ',', 'but', 'later', 'chapters', 'will', 'mix', 'both', 'styles', 'without', 'being', 'so', 'up-front', 'about', 'it', '.', 'We', 'hope', 'this', 'style', 'of', 'introduction', 'gives', 'you', 'an', 'authentic', 'taste', 'of', 'what', 'will', 'come', 'later', ',', 'while', 'covering', 'a', 'range', 'of', 'elementary', 'concepts', 'in', 'linguistics', 'and', 'computer', 'science', '.', 'If', 'you', 'have', 'basic', 'familiarity', 'with', 'both', 'areas', ',', 'you', 'can', 'skip', 'to', '5', ';', 'we', 'will', 'repeat', 'any', 'important', 'points', 'in', 'later', 'chapters', ',', 'and', 'if', 'you', 'miss', 'anything', 'you', 'can', 'easily', 'consult', 'the', 'online', 'reference', 'material', 'at', 'http', ':', '//nltk.org/genindex.html', 'and', 'nltk.org/genindex.html', 'and', 'http', ':', '//www.nltk.org/genindex.html', '.', 'If', 'the', 'material', 'is', 'completely', 'new', 'to', 'you', ',', 'this', 'chapter', 'will', 'raise', 'more', 'questions', 'than', 'it', 'answers', ',', 'questions', 'that', 'are', 'addressed', 'in', 'the', 'rest', 'of', 'this', 'book', '.']\n",
      "['I', 'wa', 'here', 'to', 'test', 'the', 'irregular', 'stem', 'word', '.', 'there', 'were', 'four', 'peopl', 'and', 'that', 'is', 'half', 'the', 'amount', 'of', 'last', 'week', ',', 'there', 'wa', 'also', 'a', 'dozen', 'peopl', 'there', '.', 'He', 'said', '``', 'after', 'he', 'would', 'train', 'and', 'get', 'the', 'train', 'home', \"''\", '.', 'No', 'previou', 'convict', ',', 'will', 'the', 'no', 'be', 'lost', 'and', 'chang', 'context', '?', 'He', 'said', 'that', 'miss', \"o'neil\", 'might', 'miss', 'the', 'train', ',', 'but', 'she', 'did', \"n't\", '.', 'He', 'wa', '12', 'years-old', ',', 'born', 'at', '12:53', 'on', 'the', '12/5/2017', '12-5-2017', 'and', '12th', 'decemb', '2017', 'thi', 'well-known', 'chapter', 'in', 'the', 'u.k', 'is', 'divid', 'into', 'up-to-d', 'section', 'that', 'skip', 'between', 'two', 'quit', 'differ', 'style', 'for', 'u.c.c', 'student', 'and', 'john', \"o'kelli\", '.', 'It', \"'s\", 'in', 'the', '``', 'comput', 'with', 'languag', \"''\", 'section', 'we', 'will', 'take', 'on', 'some', 'linguist', 'motiv', 'program', 'task', 'without', 'necessarili', 'explain', 'how', 'they', 'work', '.', 'In', 'the', '``', 'closer', 'look', 'at', 'python', \"''\", 'section', 'we', 'will', 'systemat', 'review', 'key', 'program', 'concept', '.', 'We', \"'ll\", 'flag', 'the', 'two', 'style', 'in', 'the', 'section', 'titl', ',', 'but', 'later', 'chapter', 'will', 'mix', 'both', 'style', 'without', 'be', 'so', 'up-front', 'about', 'it', '.', 'We', 'hope', 'thi', 'style', 'of', 'introduct', 'give', 'you', 'an', 'authent', 'tast', 'of', 'what', 'will', 'come', 'later', ',', 'while', 'cover', 'a', 'rang', 'of', 'elementari', 'concept', 'in', 'linguist', 'and', 'comput', 'scienc', '.', 'If', 'you', 'have', 'basic', 'familiar', 'with', 'both', 'area', ',', 'you', 'can', 'skip', 'to', '5', ';', 'we', 'will', 'repeat', 'ani', 'import', 'point', 'in', 'later', 'chapter', ',', 'and', 'if', 'you', 'miss', 'anyth', 'you', 'can', 'easili', 'consult', 'the', 'onlin', 'refer', 'materi', 'at', 'http', ':', '//nltk.org/genindex.html', 'and', 'nltk.org/genindex.html', 'and', 'http', ':', '//www.nltk.org/genindex.html', '.', 'If', 'the', 'materi', 'is', 'complet', 'new', 'to', 'you', ',', 'thi', 'chapter', 'will', 'rais', 'more', 'question', 'than', 'it', 'answer', ',', 'question', 'that', 'are', 'address', 'in', 'the', 'rest', 'of', 'thi', 'book', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nStemming the words has caused more mistakes\\n# This --> Thi\\n# Was --> wa\\n# People --> Peopl ,change --> chang\\n# divided --> divid and visited --> visit and travelled --> travel\\n# up-to-date --> up-to-d, motivated --> motiv\\n# O'Kelly --> O'Kelli , necessarily --> necessarili\\n# computing --> comput\\n# titles --> titl\\n# language --> languag\\n# systematically --> systemat\\n\""
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def openFile(file_path_name):\n",
    "\n",
    "    with open(file_path_name) as file:\n",
    "        text = file.read()\n",
    "\n",
    "    return text\n",
    "\n",
    "paragraph = openFile(\"nltk_file.txt\")\n",
    "\n",
    "token_words = nltk.word_tokenize(paragraph)\n",
    "\n",
    "# porter stemming the words\n",
    "stemmer = nltk.PorterStemmer()\n",
    "print(stemmer.stem('sleepy'))\n",
    "stem_words = [stemmer.stem(wd) for wd in token_words]\n",
    "print(token_words)\n",
    "print(stem_words)\n",
    "\n",
    "\n",
    "'''\n",
    "Stemming the words has caused more mistakes\n",
    "# This --> Thi\n",
    "# Was --> wa\n",
    "# People --> Peopl ,change --> chang\n",
    "# divided --> divid and visited --> visit and travelled --> travel\n",
    "# up-to-date --> up-to-d, motivated --> motiv\n",
    "# O'Kelly --> O'Kelli , necessarily --> necessarili\n",
    "# computing --> comput\n",
    "# titles --> titl\n",
    "# language --> languag\n",
    "# systematically --> systemat\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('wa', 'VBP'), ('here', 'RB'), ('to', 'TO'), ('test', 'VB'), ('the', 'DT'), ('irregular', 'JJ'), ('stem', 'NN'), ('word', 'NN'), ('.', '.'), ('there', 'EX'), ('were', 'VBD'), ('four', 'CD'), ('peopl', 'NN'), ('and', 'CC'), ('that', 'DT'), ('is', 'VBZ'), ('half', 'PDT'), ('the', 'DT'), ('amount', 'NN'), ('of', 'IN'), ('last', 'JJ'), ('week', 'NN'), (',', ','), ('there', 'EX'), ('wa', 'VBZ'), ('also', 'RB'), ('a', 'DT'), ('dozen', 'NN'), ('peopl', 'NN'), ('there', 'RB'), ('.', '.'), ('He', 'PRP'), ('said', 'VBD'), ('``', '``'), ('after', 'IN'), ('he', 'PRP'), ('would', 'MD'), ('train', 'VB'), ('and', 'CC'), ('get', 'VB'), ('the', 'DT'), ('train', 'NN'), ('home', 'NN'), (\"''\", \"''\"), ('.', '.'), ('No', 'DT'), ('previou', 'NN'), ('convict', 'NN'), (',', ','), ('will', 'MD'), ('the', 'DT'), ('no', 'DT'), ('be', 'VB'), ('lost', 'VBN'), ('and', 'CC'), ('chang', 'VBN'), ('context', 'NN'), ('?', '.'), ('He', 'PRP'), ('said', 'VBD'), ('that', 'IN'), ('miss', 'JJ'), (\"o'neil\", 'NN'), ('might', 'MD'), ('miss', 'VB'), ('the', 'DT'), ('train', 'NN'), (',', ','), ('but', 'CC'), ('she', 'PRP'), ('did', 'VBD'), (\"n't\", 'RB'), ('.', '.'), ('He', 'PRP'), ('wa', 'VBD'), ('12', 'CD'), ('years-old', 'JJ'), (',', ','), ('born', 'VBN'), ('at', 'IN'), ('12:53', 'CD'), ('on', 'IN'), ('the', 'DT'), ('12/5/2017', 'CD'), ('12-5-2017', 'JJ'), ('and', 'CC'), ('12th', 'CD'), ('decemb', 'NN'), ('2017', 'CD'), ('thi', 'IN'), ('well-known', 'JJ'), ('chapter', 'NN'), ('in', 'IN'), ('the', 'DT'), ('u.k', 'NN'), ('is', 'VBZ'), ('divid', 'JJ'), ('into', 'IN'), ('up-to-d', 'JJ'), ('section', 'NN'), ('that', 'WDT'), ('skip', 'VBZ'), ('between', 'IN'), ('two', 'CD'), ('quit', 'NN'), ('differ', 'NN'), ('style', 'NN'), ('for', 'IN'), ('u.c.c', 'JJ'), ('student', 'NN'), ('and', 'CC'), ('john', 'NN'), (\"o'kelli\", 'NN'), ('.', '.'), ('It', 'PRP'), (\"'s\", 'VBZ'), ('in', 'IN'), ('the', 'DT'), ('``', '``'), ('comput', 'NN'), ('with', 'IN'), ('languag', 'NN'), (\"''\", \"''\"), ('section', 'NN'), ('we', 'PRP'), ('will', 'MD'), ('take', 'VB'), ('on', 'IN'), ('some', 'DT'), ('linguist', 'JJ'), ('motiv', 'JJ'), ('program', 'NN'), ('task', 'NN'), ('without', 'IN'), ('necessarili', 'RB'), ('explain', 'VB'), ('how', 'WRB'), ('they', 'PRP'), ('work', 'VBP'), ('.', '.'), ('In', 'IN'), ('the', 'DT'), ('``', '``'), ('closer', 'JJ'), ('look', 'NN'), ('at', 'IN'), ('python', 'NN'), (\"''\", \"''\"), ('section', 'NN'), ('we', 'PRP'), ('will', 'MD'), ('systemat', 'VB'), ('review', 'VB'), ('key', 'JJ'), ('program', 'NN'), ('concept', 'NN'), ('.', '.'), ('We', 'PRP'), (\"'ll\", 'MD'), ('flag', 'VB'), ('the', 'DT'), ('two', 'CD'), ('style', 'NN'), ('in', 'IN'), ('the', 'DT'), ('section', 'NN'), ('titl', 'NN'), (',', ','), ('but', 'CC'), ('later', 'RB'), ('chapter', 'NN'), ('will', 'MD'), ('mix', 'VB'), ('both', 'DT'), ('style', 'NN'), ('without', 'IN'), ('be', 'VB'), ('so', 'RB'), ('up-front', 'JJ'), ('about', 'IN'), ('it', 'PRP'), ('.', '.'), ('We', 'PRP'), ('hope', 'VBP'), ('thi', 'JJ'), ('style', 'NN'), ('of', 'IN'), ('introduct', 'NN'), ('give', 'NN'), ('you', 'PRP'), ('an', 'DT'), ('authent', 'JJ'), ('tast', 'NN'), ('of', 'IN'), ('what', 'WP'), ('will', 'MD'), ('come', 'VB'), ('later', 'RB'), (',', ','), ('while', 'IN'), ('cover', 'VB'), ('a', 'DT'), ('rang', 'NN'), ('of', 'IN'), ('elementari', 'NN'), ('concept', 'NN'), ('in', 'IN'), ('linguist', 'NN'), ('and', 'CC'), ('comput', 'NN'), ('scienc', 'NN'), ('.', '.'), ('If', 'IN'), ('you', 'PRP'), ('have', 'VBP'), ('basic', 'VBN'), ('familiar', 'JJ'), ('with', 'IN'), ('both', 'DT'), ('area', 'NN'), (',', ','), ('you', 'PRP'), ('can', 'MD'), ('skip', 'VB'), ('to', 'TO'), ('5', 'CD'), (';', ':'), ('we', 'PRP'), ('will', 'MD'), ('repeat', 'VB'), ('ani', 'JJ'), ('import', 'NN'), ('point', 'NN'), ('in', 'IN'), ('later', 'RBR'), ('chapter', 'NN'), (',', ','), ('and', 'CC'), ('if', 'IN'), ('you', 'PRP'), ('miss', 'VBP'), ('anyth', 'IN'), ('you', 'PRP'), ('can', 'MD'), ('easili', 'VB'), ('consult', 'VB'), ('the', 'DT'), ('onlin', 'NN'), ('refer', 'NN'), ('materi', 'NN'), ('at', 'IN'), ('http', 'NN'), (':', ':'), ('//nltk.org/genindex.html', 'NN'), ('and', 'CC'), ('nltk.org/genindex.html', 'NN'), ('and', 'CC'), ('http', 'NN'), (':', ':'), ('//www.nltk.org/genindex.html', 'NN'), ('.', '.'), ('If', 'IN'), ('the', 'DT'), ('materi', 'NN'), ('is', 'VBZ'), ('complet', 'JJ'), ('new', 'JJ'), ('to', 'TO'), ('you', 'PRP'), (',', ','), ('thi', 'VBP'), ('chapter', 'NN'), ('will', 'MD'), ('rais', 'VB'), ('more', 'JJR'), ('question', 'NN'), ('than', 'IN'), ('it', 'PRP'), ('answer', 'VBZ'), (',', ','), ('question', 'NN'), ('that', 'WDT'), ('are', 'VBP'), ('address', 'RB'), ('in', 'IN'), ('the', 'DT'), ('rest', 'NN'), ('of', 'IN'), ('thi', 'JJ'), ('book', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "pos_tagger_words = nltk.pos_tag(stem_words)\n",
    "print(pos_tagger_words )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "wa\n",
      "here\n",
      "to\n",
      "test\n",
      "the\n",
      "irregular\n",
      "stemming\n",
      "word\n",
      ".\n",
      "There\n",
      "were\n",
      "four\n",
      "people\n",
      "and\n",
      "that\n",
      "is\n",
      "half\n",
      "the\n",
      "amount\n",
      "of\n",
      "last\n",
      "week\n",
      ",\n",
      "there\n",
      "wa\n",
      "also\n",
      "a\n",
      "dozen\n",
      "people\n",
      "there\n",
      ".\n",
      "He\n",
      "said\n",
      "``\n",
      "after\n",
      "he\n",
      "would\n",
      "train\n",
      "and\n",
      "get\n",
      "the\n",
      "train\n",
      "home\n",
      "''\n",
      ".\n",
      "No\n",
      "previous\n",
      "conviction\n",
      ",\n",
      "will\n",
      "the\n",
      "no\n",
      "be\n",
      "lost\n",
      "and\n",
      "change\n",
      "context\n",
      "?\n",
      "He\n",
      "said\n",
      "that\n",
      "miss\n",
      "O'Neill\n",
      "might\n",
      "miss\n",
      "the\n",
      "train\n",
      ",\n",
      "but\n",
      "she\n",
      "did\n",
      "n't\n",
      ".\n",
      "He\n",
      "wa\n",
      "12\n",
      "years-old\n",
      ",\n",
      "born\n",
      "at\n",
      "12:53\n",
      "on\n",
      "the\n",
      "12/5/2017\n",
      "12-5-2017\n",
      "and\n",
      "12th\n",
      "December\n",
      "2017\n",
      "This\n",
      "well-known\n",
      "chapter\n",
      "in\n",
      "the\n",
      "U.K\n",
      "is\n",
      "divided\n",
      "into\n",
      "up-to-date\n",
      "section\n",
      "that\n",
      "skip\n",
      "between\n",
      "two\n",
      "quite\n",
      "different\n",
      "style\n",
      "for\n",
      "U.C.C\n",
      "student\n",
      "and\n",
      "John\n",
      "O'Kelly\n",
      ".\n",
      "It\n",
      "'s\n",
      "in\n",
      "the\n",
      "``\n",
      "computing\n",
      "with\n",
      "language\n",
      "''\n",
      "section\n",
      "we\n",
      "will\n",
      "take\n",
      "on\n",
      "some\n",
      "linguistically\n",
      "motivated\n",
      "programming\n",
      "task\n",
      "without\n",
      "necessarily\n",
      "explaining\n",
      "how\n",
      "they\n",
      "work\n",
      ".\n",
      "In\n",
      "the\n",
      "``\n",
      "closer\n",
      "look\n",
      "at\n",
      "Python\n",
      "''\n",
      "section\n",
      "we\n",
      "will\n",
      "systematically\n",
      "review\n",
      "key\n",
      "programming\n",
      "concept\n",
      ".\n",
      "We\n",
      "'ll\n",
      "flag\n",
      "the\n",
      "two\n",
      "style\n",
      "in\n",
      "the\n",
      "section\n",
      "title\n",
      ",\n",
      "but\n",
      "later\n",
      "chapter\n",
      "will\n",
      "mix\n",
      "both\n",
      "style\n",
      "without\n",
      "being\n",
      "so\n",
      "up-front\n",
      "about\n",
      "it\n",
      ".\n",
      "We\n",
      "hope\n",
      "this\n",
      "style\n",
      "of\n",
      "introduction\n",
      "give\n",
      "you\n",
      "an\n",
      "authentic\n",
      "taste\n",
      "of\n",
      "what\n",
      "will\n",
      "come\n",
      "later\n",
      ",\n",
      "while\n",
      "covering\n",
      "a\n",
      "range\n",
      "of\n",
      "elementary\n",
      "concept\n",
      "in\n",
      "linguistics\n",
      "and\n",
      "computer\n",
      "science\n",
      ".\n",
      "If\n",
      "you\n",
      "have\n",
      "basic\n",
      "familiarity\n",
      "with\n",
      "both\n",
      "area\n",
      ",\n",
      "you\n",
      "can\n",
      "skip\n",
      "to\n",
      "5\n",
      ";\n",
      "we\n",
      "will\n",
      "repeat\n",
      "any\n",
      "important\n",
      "point\n",
      "in\n",
      "later\n",
      "chapter\n",
      ",\n",
      "and\n",
      "if\n",
      "you\n",
      "miss\n",
      "anything\n",
      "you\n",
      "can\n",
      "easily\n",
      "consult\n",
      "the\n",
      "online\n",
      "reference\n",
      "material\n",
      "at\n",
      "http\n",
      ":\n",
      "//nltk.org/genindex.html\n",
      "and\n",
      "nltk.org/genindex.html\n",
      "and\n",
      "http\n",
      ":\n",
      "//www.nltk.org/genindex.html\n",
      ".\n",
      "If\n",
      "the\n",
      "material\n",
      "is\n",
      "completely\n",
      "new\n",
      "to\n",
      "you\n",
      ",\n",
      "this\n",
      "chapter\n",
      "will\n",
      "raise\n",
      "more\n",
      "question\n",
      "than\n",
      "it\n",
      "answer\n",
      ",\n",
      "question\n",
      "that\n",
      "are\n",
      "addressed\n",
      "in\n",
      "the\n",
      "rest\n",
      "of\n",
      "this\n",
      "book\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Leminization\n",
    "pos_words = nltk.pos_tag(token_words)\n",
    "# lem_words = nltk.ne_chunk(pos_words, binary=True)\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "for word in token_words:\n",
    "    lem_words = wn.lemmatize(word)\n",
    "    print(lem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This --> Thi in which the pos-tagging would incorrectly classify the word as a 'IN' \n",
    "# Was --> wa\n",
    "# People --> Peopl ,change --> chang\n",
    "# divided --> divid and visited --> visit and travelled --> travel\n",
    "# up-to-date --> up-to-d, motivated --> motiv\n",
    "# O'Kelly --> O'Kelli , necessarily --> necessarili\n",
    "# computing --> comput\n",
    "# titles --> titl\n",
    "# language --> languag\n",
    "# systematically --> systemat\n",
    "# December --> decemb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The porters stemming method was often crude in its approach. However, more times than not it would achieved its goal. This was normally conducted by removing the derivational affixes. There were some possitive results in which past tense words like visited would be changed to visit. But basic words such as \"this\" would be truncated to \"thi\" and \"up-to-date\" was converted to \"up-to-d\". \n",
    "\n",
    "To see the effect this had on the meaning of the word, the POS-tagging was performed on the remaining stemmed words.  This showed the changed of lexical catagorry of words like \"This\" to be categorised as \"IN\", which is incorrect. Looking at the source code used for the porter stemming within the nltk package [http://www.nltk.org/_modules/nltk/stem/porter.html], it can be seen on line 293 shown below why this has occured. The code simply looks for instances of words ending in s and replaces it was nothing.\n",
    "\n",
    "        return self._apply_rule_list(word, [\n",
    "            ('sses', 'ss', None), # SSES -> SS\n",
    "            ('ies', 'i', None),   # IES  -> I\n",
    "            ('ss', 'ss', None),   # SS   -> SS\n",
    "            ('s', '', None),      # S    ->\n",
    "        ])\n",
    " \n",
    "Other oddities included words ending in y being converted to i's such as kelly --> kelli, the removal of \"ing\" and \"er\" from words but not replacing with 'e's where applicable, such as computing --> comput and December --> Decemb.\n",
    "\n",
    "Lemiziation on the other hand was much more "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wa\n",
      "be\n"
     ]
    }
   ],
   "source": [
    "computing = nltk.stem.WordNetLemmatizer().lemmatize('was')\n",
    "print(computing)\n",
    "verb_computing = nltk.stem.WordNetLemmatizer().lemmatize('was', 'v')\n",
    "print(verb_computing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:comp47350]",
   "language": "python",
   "name": "conda-env-comp47350-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
